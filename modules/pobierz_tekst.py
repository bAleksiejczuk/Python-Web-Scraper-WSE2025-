import requests
from bs4 import BeautifulSoup
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# ============================================================================
# KONFIGURACJA
# ============================================================================
print("="*60)
print("ğŸ“ EKSTRAKTOR TEKSTU")
print("="*60)

input_file = input("\nğŸ“„ Plik z linkami (Enter = all_links.txt): ").strip()
input_file = input_file if input_file else "all_links.txt"

output_file = input("ğŸ’¾ Plik wyjÅ›ciowy (Enter = teksty.txt): ").strip()
output_file = output_file if output_file else "teksty.txt"

max_workers = input("ğŸ”§ WÄ…tkÃ³w (Enter = 10): ").strip()
max_workers = int(max_workers) if max_workers else 10

delay = input("â±ï¸  OpÃ³Åºnienie miÄ™dzy requestami w sekundach (Enter = 0.5): ").strip()
delay = float(delay) if delay else 0.5

# ============================================================================
# INICJALIZACJA
# ============================================================================
separator = "_" * 50
write_lock = Lock()

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

print(f"\nğŸš€ Rozpoczynam ekstrakcjÄ™...")
print(f"ğŸ“„ Å¹rÃ³dÅ‚o: {input_file}")
print(f"ğŸ’¾ Cel: {output_file}")
print(f"ğŸ”§ WÄ…tkÃ³w: {max_workers}")
print(f"â±ï¸  OpÃ³Åºnienie: {delay}s\n")

start_time = time.time()

# ============================================================================
# FUNKCJA POBIERANIA TEKSTU
# ============================================================================
def get_text_from_url(url):
    """Pobiera tekst ze strony"""
    
    # OpÃ³Åºnienie
    time.sleep(delay)
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # UsuÅ„ niewidoczne elementy
        for element in soup(['script', 'style', 'head', 'title', 'meta', 'iframe', 'noscript']):
            element.extract()
        
        # Formatowanie
        for br in soup.find_all('br'):
            br.replace_with('\n')
        
        for p in soup.find_all('p'):
            p.append(soup.new_string('\n\n'))
        
        for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            h.append(soup.new_string('\n\n'))
        
        for li in soup.find_all('li'):
            li.insert(0, soup.new_string('â€¢ '))
            li.append(soup.new_string('\n'))
        
        text = soup.get_text()
        
        # Czyszczenie
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(line for line in lines if line)
        
        return text
    
    except Exception as e:
        return f"BÅÄ„D PODCZAS POBIERANIA STRONY: {str(e)}"

# ============================================================================
# FUNKCJA PRZETWARZANIA LINKU
# ============================================================================
def process_link(link, index, total):
    """Przetwarza pojedynczy link"""
    print(f"ğŸ” [{index}/{total}] {link}")
    text = get_text_from_url(link)
    return link, text

# ============================================================================
# WCZYTAJ LINKI
# ============================================================================
try:
    with open(input_file, 'r', encoding='utf-8') as f:
        links = [line.strip() for line in f if line.strip()]
    
    total_links = len(links)
    print(f"ğŸ“‹ Znaleziono {total_links} linkÃ³w\n")

except FileNotFoundError:
    print(f"âŒ BÅ‚Ä…d: Plik '{input_file}' nie istnieje!")
    exit(1)

# ============================================================================
# PRZETWARZANIE WIELOWÄ„TKOWE
# ============================================================================
with open(output_file, 'w', encoding='utf-8') as out_file:
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Uruchom wszystkie zadania
        futures = {
            executor.submit(process_link, link, i+1, total_links): link 
            for i, link in enumerate(links)
        }
        
        # Zbieraj wyniki
        completed = 0
        for future in as_completed(futures):
            completed += 1
            try:
                link, text = future.result()
                
                # Zapisz do pliku
                with write_lock:
                    out_file.write(f"{link}\n\n")
                    out_file.write(f"{text}\n\n")
                    out_file.write(f"{separator}\n\n")
                    out_file.flush()
                
                print(f"âœ… [{completed}/{total_links}] Zapisano")
                
            except Exception as e:
                link = futures[future]
                print(f"âŒ BÅ‚Ä…d: {link} - {e}")

# ============================================================================
# PODSUMOWANIE
# ============================================================================
elapsed = time.time() - start_time

print(f"\n" + "="*60)
print(f"âœ… ZAKOÅƒCZONO")
print(f"="*60)
print(f"â±ï¸  Czas: {elapsed:.2f}s")
print(f"ğŸ“Š Przetworzone: {total_links} linkÃ³w")
print(f"âš¡ PrÄ™dkoÅ›Ä‡: {total_links/elapsed:.2f} linkÃ³w/s")
print(f"="*60)
print(f"\nğŸ’¾ Wyniki zapisano w: {output_file}")
print(f"ğŸ‰ GOTOWE!")